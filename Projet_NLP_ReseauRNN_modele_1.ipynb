{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet_NLP_ReseauRNN_modele_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Na3pHouZ01S"
      },
      "source": [
        "# 1. Importation des librairies et chargement du dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvD6KRau_STo",
        "outputId": "e096afe2-97d1-4bbf-c9f0-7d690e1e3b8c"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from keras.layers import LSTM, Activation, Dropout, Dense, Input, Masking, Embedding, Flatten\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.models import Model\r\n",
        "import string\r\n",
        "import re\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import keras\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn import linear_model\r\n",
        "from keras.models import Sequential\r\n",
        "import pickle\r\n",
        "\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "import requests\r\n",
        "import json\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NotBIx-U_rz4",
        "outputId": "caa72968-6875-45a8-c654-2bc4f8a9b609"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q2PaqXo9kkZ"
      },
      "source": [
        "#Selection de la data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWyXsJ6TK83k"
      },
      "source": [
        "data = pd.read_csv('gdrive/MyDrive/Projet_NLP/dataset.csv',encoding='ISO-8859-1',header=None,names=['target','ids','date','flag','user','text'])\r\n",
        "\r\n",
        "dataset_neg=data['text'][:10000]\r\n",
        "dataset_pos=data['text'][800000:810000]\r\n",
        "\r\n",
        "sentiment_neg=data['target'][:10000]\r\n",
        "sentiment_pos=data['target'][800000:810000]\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fgwj4sh9o-t"
      },
      "source": [
        "#Mise en DataFrame + suppression de la ponctuation superflue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CmBu6EwfLD6h",
        "outputId": "e98804b3-fd4f-468e-855b-b4b28a96c377"
      },
      "source": [
        "def to_tab(data):\r\n",
        "  sentiment_tab=[]\r\n",
        "  for string_line in data:\r\n",
        "    sentiment_tab.append(string_line)\r\n",
        "\r\n",
        "  return sentiment_tab\r\n",
        "\r\n",
        "dataset_sentiment_pos = to_tab(sentiment_pos)\r\n",
        "dataset_sentiment_neg = to_tab(sentiment_neg)\r\n",
        "\r\n",
        "dataset_sentiment= dataset_sentiment_pos + dataset_sentiment_neg\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def dataset_nopunct(data):\r\n",
        "  punct = string.punctuation\r\n",
        "  tab_punct=[]\r\n",
        "  for i in range(len(punct)):\r\n",
        "    tab_punct.append(punct[i])\r\n",
        "\r\n",
        "\r\n",
        "  no_punct_dataset=[]\r\n",
        "  element=\"\"\r\n",
        "  for elt in data:\r\n",
        "    for letter in elt:\r\n",
        "      if letter not in tab_punct and not letter.isdigit():\r\n",
        "        element += letter\r\n",
        "\r\n",
        "    no_punct_dataset.append(element)\r\n",
        "    element =\"\"\r\n",
        "  \r\n",
        "  return no_punct_dataset\r\n",
        "\r\n",
        "datasetPosNoPunct=dataset_nopunct(dataset_pos)\r\n",
        "datasetNegNoPunct=dataset_nopunct(dataset_neg)\r\n",
        "\r\n",
        "\r\n",
        "datasetNoPunct=dataset_nopunct(dataset_pos) +  dataset_nopunct(dataset_neg)\r\n",
        "tab=pd.DataFrame(datasetNoPunct, columns = [\"text\"])\r\n",
        "tab[\"sentiment\"] = dataset_sentiment\r\n",
        "tab"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I LOVE HealthUandPets u guys r the best</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>im meeting up with one of my besties tonight C...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DaRealSunisaKim Thanks for the Twitter add Sun...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Being sick can be really cheap when it hurts t...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LovesBrooklyn he has that effect on everyone</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>Aww thats sad</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>stupid dvds stuffing up the good bits in jaws</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>DandySephy No Only close friends and family Im...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>CRAP After looking when I last tweeted WHY AM ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>Its Another Rainboot day</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  sentiment\n",
              "0               I LOVE HealthUandPets u guys r the best           4\n",
              "1      im meeting up with one of my besties tonight C...          4\n",
              "2      DaRealSunisaKim Thanks for the Twitter add Sun...          4\n",
              "3      Being sick can be really cheap when it hurts t...          4\n",
              "4          LovesBrooklyn he has that effect on everyone           4\n",
              "...                                                  ...        ...\n",
              "19995                                     Aww thats sad           0\n",
              "19996     stupid dvds stuffing up the good bits in jaws           0\n",
              "19997  DandySephy No Only close friends and family Im...          0\n",
              "19998  CRAP After looking when I last tweeted WHY AM ...          0\n",
              "19999                          Its Another Rainboot day           0\n",
              "\n",
              "[20000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMfm0JSb99co"
      },
      "source": [
        "#Mise en minuscule des tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "_o3ww7_tLGHm",
        "outputId": "a2af4b4b-9fc4-4414-dcd1-afb9667502ae"
      },
      "source": [
        "def to_lower(data):\r\n",
        "  for i in range(len(data[\"text\"])):\r\n",
        "    data[\"text\"][i]=data[\"text\"][i].lower()\r\n",
        "  \r\n",
        "  return data\r\n",
        "\r\n",
        "dataset_lower=to_lower(tab)\r\n",
        "dataset_lower"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i love healthuandpets u guys r the best</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>im meeting up with one of my besties tonight c...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>darealsunisakim thanks for the twitter add sun...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>being sick can be really cheap when it hurts t...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lovesbrooklyn he has that effect on everyone</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>aww thats sad</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>stupid dvds stuffing up the good bits in jaws</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>dandysephy no only close friends and family im...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>crap after looking when i last tweeted why am ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>its another rainboot day</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  sentiment\n",
              "0               i love healthuandpets u guys r the best           4\n",
              "1      im meeting up with one of my besties tonight c...          4\n",
              "2      darealsunisakim thanks for the twitter add sun...          4\n",
              "3      being sick can be really cheap when it hurts t...          4\n",
              "4          lovesbrooklyn he has that effect on everyone           4\n",
              "...                                                  ...        ...\n",
              "19995                                     aww thats sad           0\n",
              "19996     stupid dvds stuffing up the good bits in jaws           0\n",
              "19997  dandysephy no only close friends and family im...          0\n",
              "19998  crap after looking when i last tweeted why am ...          0\n",
              "19999                          its another rainboot day           0\n",
              "\n",
              "[20000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZRAQWqz-DCt"
      },
      "source": [
        "#Tokenization des tweets avec keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLHojs_-LGgg",
        "outputId": "e19020d9-a73b-4e46-fb10-a58059633127"
      },
      "source": [
        "#The Tokenizer class of Keras is used for vectorizing a text corpus.\r\n",
        "#For this either, each text input is converted into integer sequence or a vector that has a coefficient for each token in the form of binary values.\r\n",
        "tokenizer = Tokenizer(num_words=20000, split=\" \")\r\n",
        "\r\n",
        "#The fit_on_texts method is a part of Keras tokenizer class which is used to update the internal vocabulary for the texts list.\r\n",
        "#We need to call be before using other methods of texts_to_sequences or texts_to_matrix.\r\n",
        "tokenizer.fit_on_texts(dataset_lower[\"text\"].values)\r\n",
        "\r\n",
        "#texts_to_sequences method helps in converting tokens of text corpus into a sequence of integers.\r\n",
        "X= tokenizer.texts_to_sequences(dataset_lower[\"text\"].values)\r\n",
        "\r\n",
        "#pad_sequence takes a LIST of sequences as an input (list of list) and will return a list of padded sequences (all the same length).\r\n",
        "X = pad_sequences(X)\r\n",
        "print(X[:33])\r\n",
        "print(X.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...  388    3  179]\n",
            " [   0    0    0 ...  142  294  324]\n",
            " [   0    0    0 ...  131    4 2092]\n",
            " ...\n",
            " [   0    0    0 ...   25    3  422]\n",
            " [   0    0    0 ...  712   18  109]\n",
            " [   0    0    0 ... 9500 9501 9502]]\n",
            "(20000, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsiW1yoXvK_5",
        "outputId": "7e75bbc9-537b-4020-bc4c-8c75ec517ecc"
      },
      "source": [
        "# 4 --> [0 1] pour les positifs et 0 --> [1 0] pour les négatifs\r\n",
        "Y=pd.get_dummies(tab[\"sentiment\"]).values\r\n",
        "print(Y)\r\n",
        "print(Y.shape)\r\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " ...\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]]\n",
            "(20000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIAUZEifAGd8"
      },
      "source": [
        "##Modèle de RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLyRJpb5AHO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c3094c-8a92-4a60-f81f-3ab09feeb434"
      },
      "source": [
        "\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "#Transforme les entiers positifs (index) en vecteurs de taille fixe.\r\n",
        "#La première étape de l'utilisation d'une couche d'incorporation consiste à encoder cette phrase par des indices.\r\n",
        "model.add(Embedding(20000, 256, input_length=X.shape[1]))\r\n",
        "model.add(Dropout(0.3))\r\n",
        "\r\n",
        "#Les réseaux de longue mémoire à court terme (LSTM) sont une extension pour les réseaux neuronaux récurrents, qui étend leur mémoire.\r\n",
        "#Par conséquent, il est bien adapté pour apprendre des expériences importantes qui ont des retards très longs.\r\n",
        "model.add(LSTM(256,return_sequences=False,dropout=0.3,recurrent_dropout=0.2))\r\n",
        "\r\n",
        "#Sigmoïde: produit une courbe en forme de S.\r\n",
        "#Bien que de nature non linéaire, il ne tient toutefois pas compte des légères variations des entrées, ce qui entraîne des résultats similaires.\r\n",
        "model.add(Dense(2, activation=\"sigmoid\"))\r\n",
        "\r\n",
        "model.compile(\r\n",
        "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "#On split la data, 70% pour l'entrainement et le reste pour la validation\r\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,Y,train_size=0.7,random_state=0)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 32, 256)           5120000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 514       \n",
            "=================================================================\n",
            "Total params: 5,645,826\n",
            "Trainable params: 5,645,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KcxXcKY-Rw9"
      },
      "source": [
        "##Entrainement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ncgIAQnAPN3",
        "outputId": "0e39605d-9c7a-47a1-ae08-bdd7e3ed0f3d"
      },
      "source": [
        "model.fit(x_train,  y_train, \r\n",
        "                   epochs=8, batch_size=100,verbose =2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "140/140 - 87s - loss: 0.5860 - accuracy: 0.6859\n",
            "Epoch 2/8\n",
            "140/140 - 85s - loss: 0.4106 - accuracy: 0.8212\n",
            "Epoch 3/8\n",
            "140/140 - 85s - loss: 0.2973 - accuracy: 0.8811\n",
            "Epoch 4/8\n",
            "140/140 - 85s - loss: 0.2076 - accuracy: 0.9202\n",
            "Epoch 5/8\n",
            "140/140 - 85s - loss: 0.1412 - accuracy: 0.9452\n",
            "Epoch 6/8\n",
            "140/140 - 86s - loss: 0.1059 - accuracy: 0.9593\n",
            "Epoch 7/8\n",
            "140/140 - 86s - loss: 0.0816 - accuracy: 0.9686\n",
            "Epoch 8/8\n",
            "140/140 - 86s - loss: 0.0630 - accuracy: 0.9774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f723af1ae90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5WoZBMobFgb"
      },
      "source": [
        "#Chargement du modèle déjà entrainé"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ0vFsW3bE34"
      },
      "source": [
        "#model_1 = keras.models.load_model('/content/gdrive/MyDrive/Projet_NLP/Reseau_RNN_model_1/Model_1')\r\n",
        "#model_1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKXq8XMV-WBP"
      },
      "source": [
        "##Evaluation du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JMbmdtCBJkj",
        "outputId": "c90d6edc-60b4-4ce2-a4de-4bc8c862f7ec"
      },
      "source": [
        "model.evaluate(x_test,y_test)\r\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "188/188 [==============================] - 8s 43ms/step - loss: 1.1008 - accuracy: 0.7232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.100848913192749, 0.7231666445732117]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBOfKw01-YYR"
      },
      "source": [
        "##Sauvegarde du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC_tyOaIoaIo",
        "outputId": "d685ac2d-1669-4332-d510-2093356d4a50"
      },
      "source": [
        "#model.save('/content/gdrive/MyDrive/Projet_NLP/Reseau_RNN_models/Model_1')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Projet_NLP/Reseau_RNN_models/Model_1/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Cbv1SKF-bEe"
      },
      "source": [
        "##Récupération de reviews depuis l'API Yelp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9nDKFfBtXdB",
        "outputId": "2ac61874-4666-468e-d14c-6365cb6b892e"
      },
      "source": [
        "api_key='q1HJoEmRMdypie_E6lYwiih0hNfuSatSTQXH6Bh9fnltm30JX4XO1TSCd6NARRVszrs4lMXb7N3i6LWB_UIBEinyG_ah-0expG6GUnzTdUss2gvNQRcJ7fJhK5g2YHYx'\r\n",
        "headers = {'Authorization': 'Bearer %s' % api_key}\r\n",
        "\r\n",
        "\r\n",
        "url = \"https://api.yelp.com/v3/businesses/FEVQpbOPOwAPNIgO7D3xxw/reviews\"\r\n",
        "req = requests.get(url, headers=headers)\r\n",
        "print('the status code is {}'.format(req.status_code))\r\n",
        "\r\n",
        "\r\n",
        "data=json.loads(req.text)\r\n",
        "data"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the status code is 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'possible_languages': ['fr', 'en', 'zh', 'pt', 'de', 'it', 'sv', 'ja', 'es'],\n",
              " 'reviews': [{'id': 'qOiuCTz_Gzgmose1GHaAlg',\n",
              "   'rating': 5,\n",
              "   'text': \"Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\",\n",
              "   'time_created': '2020-12-19 07:52:35',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=qOiuCTz_Gzgmose1GHaAlg&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': 'FRKnLBnlFiasr1Dc8oOIGQ',\n",
              "    'image_url': 'https://s3-media3.fl.yelpcdn.com/photo/1wm0yjWSw92j_ZsUFZBGzQ/o.jpg',\n",
              "    'name': 'Sarah G.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=FRKnLBnlFiasr1Dc8oOIGQ'}},\n",
              "  {'id': '7d56A_ObMPHHyywcfhnrUw',\n",
              "   'rating': 5,\n",
              "   'text': 'Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...',\n",
              "   'time_created': '2021-02-25 19:56:35',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=7d56A_ObMPHHyywcfhnrUw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': '77MIRZBZqbQKoonun8qWjQ',\n",
              "    'image_url': 'https://s3-media2.fl.yelpcdn.com/photo/ptL063kjqRUDQ6pny96iqg/o.jpg',\n",
              "    'name': 'Ipsita D.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=77MIRZBZqbQKoonun8qWjQ'}},\n",
              "  {'id': 'SYykUNBT2dJznMV5PpRnhg',\n",
              "   'rating': 5,\n",
              "   'text': 'Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...',\n",
              "   'time_created': '2020-12-12 15:19:49',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=SYykUNBT2dJznMV5PpRnhg&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': 'yw2cJk_SfGZlcoZKEUevxw',\n",
              "    'image_url': 'https://s3-media1.fl.yelpcdn.com/photo/EH2WTffzgKs72GHqa4kn6A/o.jpg',\n",
              "    'name': 'Thomas V.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=yw2cJk_SfGZlcoZKEUevxw'}}],\n",
              " 'total': 5609}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6dai3p_-k5j"
      },
      "source": [
        "#Traitement de nos futurs tweets/reviews qui nous serviront d'inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbYYiUOtYtSw"
      },
      "source": [
        "def dataset_nopunct(data):\r\n",
        "    punct = string.punctuation\r\n",
        "    tab_punct=[]\r\n",
        "    for i in range(len(punct)):\r\n",
        "      tab_punct.append(punct[i])\r\n",
        "\r\n",
        "\r\n",
        "    no_punct_dataset=[]\r\n",
        "    element=\"\"\r\n",
        "  \r\n",
        "    for letter in data:\r\n",
        "      if letter not in tab_punct and not letter.isdigit():\r\n",
        "        element += letter\r\n",
        "\r\n",
        "    no_punct_dataset.append(element)\r\n",
        "    element =\"\"\r\n",
        "    \r\n",
        "    return no_punct_dataset\r\n",
        "    \r\n",
        "def traitement(my_string):\r\n",
        "    \r\n",
        "  dataset_string=dataset_nopunct(my_string)\r\n",
        "\r\n",
        "  tab_string=pd.DataFrame(dataset_string, columns = [\"text\"])\r\n",
        "\r\n",
        "  test_lower=to_lower(tab_string)\r\n",
        "\r\n",
        "  tokenizer = Tokenizer(num_words=20000, split=\" \")\r\n",
        "  tokenizer.fit_on_texts(test_lower[\"text\"].values)\r\n",
        "\r\n",
        "  X_string= tokenizer.texts_to_sequences(test_lower[\"text\"].values)\r\n",
        "  X_string = pad_sequences(X_string)\r\n",
        "\r\n",
        "  return X_string\r\n",
        "\r\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRk1dKHL-xG2"
      },
      "source": [
        "#Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UGGzXlNYCOu",
        "outputId": "f9babca2-4f3c-4cb9-ca7e-d31c8e238c53"
      },
      "source": [
        "str1 =data[\"reviews\"][0][\"text\"]\r\n",
        "str2=\"I ordered a burger and milk shake thru Uber eats, only took a few bites of burger fries and a few sips of milk shake and got extremely ill after taking a few bites of the burger and a few sips of milkshake. My body could tell something was off and started feeling nauseous, had a horrible pounding headache and was violently vomiting.  I don't know if this was food poisoning or the food was tampered with , I   Will not be ordering from here again. Don't eat fast food often , but have never had a issue with shack shack before the other times I had it.\"\r\n",
        "str3=\"Buying beware...these are addicting.Still the best doughnuts in my opinion in the city. Ginormous and fluffy like a pillow but somehow still a substantial bite.All doughnuts are scrumptious but greatly enjoyed the Dulce De De Leche in the batch we tried the best.Big win, you can order on Seamless for delivery also on the website they say Caviar, Doordash, Uber Eats and Grub Hub and Sweetist are among other sites for delivery as well.\"\r\n",
        "\r\n",
        "\r\n",
        "test1 = traitement(str1)\r\n",
        "test2 = traitement(str2)\r\n",
        "test3 = traitement(str3)\r\n",
        "\r\n",
        "\r\n",
        "preds1 = model.predict(test1)\r\n",
        "preds2 = model.predict(test2)\r\n",
        "preds3 = model.predict(test3)\r\n",
        "print(preds1)\r\n",
        "print(preds2)\r\n",
        "print(preds3)\r\n",
        "\r\n",
        "\r\n",
        "prediction_list=[preds1,preds2,preds3]\r\n",
        "sentiment_target=[\"positif\",\"negatif\",\"positif\"]\r\n",
        "\r\n",
        "for i in range(len(prediction_list)):\r\n",
        "  if prediction_list[i][0][0] < (1/3):\r\n",
        "    print('predicted sentiment : negatif')\r\n",
        "    print(f\"target sentiment : {sentiment_target[i]}\")\r\n",
        "  elif  prediction_list[i][0][0] > (2/3): \r\n",
        "    print('predicted sentiment : positif')\r\n",
        "    print(f\"target sentiment : {sentiment_target[i]}\")\r\n",
        "  else:\r\n",
        "    print('predicted sentiment : neutre')\r\n",
        "    print(f\"target sentiment : {sentiment_target[i]}\")\r\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.9841034  0.01516014]]\n",
            "[[0.10678524 0.88083184]]\n",
            "[[0.88095653 0.12048572]]\n",
            "predicted sentiment : positif\n",
            "target sentiment : positif\n",
            "predicted sentiment : negatif\n",
            "target sentiment : negatif\n",
            "predicted sentiment : positif\n",
            "target sentiment : positif\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}