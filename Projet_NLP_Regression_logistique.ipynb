{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet_NLP_Regression_logistique.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KLgQ2ZJ7I76"
      },
      "source": [
        "# 1. Importation des librairies et chargement du dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIfcgUOoKBEa",
        "outputId": "3faf9be7-f87b-4073-a812-554e683ab13b"
      },
      "source": [
        "\r\n",
        "from keras.datasets import imdb\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from keras.layers import LSTM, Activation, Dropout, Dense, Input, Masking, Embedding, Flatten\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.models import Model\r\n",
        "import string\r\n",
        "import re\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import keras\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn import linear_model\r\n",
        "from keras.models import Sequential\r\n",
        "import pickle\r\n",
        "\r\n",
        "import requests\r\n",
        "import json\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz6Cz2RTpKtV",
        "outputId": "7fae2783-6ab0-44ba-8cbe-83e49aa9e32b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "555qv9HPoyq1"
      },
      "source": [
        "data = pd.read_csv('gdrive/MyDrive/Projet_NLP/dataset.csv',encoding='ISO-8859-1',header=None,names=['target','ids','date','flag','user','text'])\r\n",
        "\r\n",
        "dataset_neg=data['text'][:10000]\r\n",
        "dataset_pos=data['text'][800000:810000]\r\n",
        "\r\n",
        "sentiment_neg=data['target'][:10000]\r\n",
        "sentiment_pos=data['target'][800000:810000]\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w95jO_1G7U23"
      },
      "source": [
        "# 2. Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap68JTdKpugo"
      },
      "source": [
        "##Mettre en minuscule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFUC1D_npsVG"
      },
      "source": [
        "\r\n",
        "def to_lower(data):\r\n",
        "  tab_lower=[]\r\n",
        "  for string in data:\r\n",
        "    tab_lower.append(string.lower())\r\n",
        "  \r\n",
        "  return tab_lower\r\n",
        "\r\n",
        "dataset_pos_lower=to_lower(dataset_pos)\r\n",
        "dataset_neg_lower=to_lower(dataset_neg)\r\n",
        "\r\n",
        "#print(dataset_pos_lower[:5])\r\n",
        "#print(dataset_neg_lower[:5])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3l4VSEdp6_n"
      },
      "source": [
        "##Enlever la ponctuation et les chiffres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHkCbyVV6_RJ"
      },
      "source": [
        "\r\n",
        "def to_tab(data):\r\n",
        "  sentiment_tab=[]\r\n",
        "  for string_line in data:\r\n",
        "    sentiment_tab.append(string_line)\r\n",
        "\r\n",
        "  return sentiment_tab\r\n",
        "\r\n",
        "dataset_sentiment_pos = to_tab(sentiment_pos)\r\n",
        "dataset_sentiment_neg = to_tab(sentiment_neg)\r\n",
        "\r\n",
        "dataset_sentiment= dataset_sentiment_pos + dataset_sentiment_neg\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def dataset_nopunct(data):\r\n",
        "  punct = string.punctuation\r\n",
        "  tab_punct=[]\r\n",
        "  for i in range(len(punct)):\r\n",
        "    tab_punct.append(punct[i])\r\n",
        "\r\n",
        "\r\n",
        "  no_punct_dataset=[]\r\n",
        "  element=\"\"\r\n",
        "  for elt in data:\r\n",
        "    for letter in elt:\r\n",
        "      if letter not in tab_punct and not letter.isdigit():\r\n",
        "        element += letter\r\n",
        "\r\n",
        "    no_punct_dataset.append(element)\r\n",
        "    element =\"\"\r\n",
        "  \r\n",
        "  return no_punct_dataset\r\n",
        "\r\n",
        "datasetPosNoPunct=dataset_nopunct(dataset_pos_lower)\r\n",
        "datasetNegNoPunct=dataset_nopunct(dataset_neg_lower)\r\n",
        "\r\n",
        "#print(datasetPosNoPunct[:5])\r\n",
        "#print(datasetNegNoPunct[:5])\r\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OGlyH00qDR5"
      },
      "source": [
        "##Tokenization des tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xtPAcyMDi6C"
      },
      "source": [
        "\r\n",
        "def Tokenize(data):\r\n",
        "  tab_traitement=[]\r\n",
        "  for elt in data:\r\n",
        "    tab_traitement.append(nltk.word_tokenize(elt))\r\n",
        "  \r\n",
        "  return tab_traitement\r\n",
        "\r\n",
        "dataset_pos_tokenize = Tokenize(datasetPosNoPunct)\r\n",
        "dataset_neg_tokenize = Tokenize(datasetNegNoPunct)\r\n",
        "\r\n",
        "#print(dataset_pos_tokenize[:2])\r\n",
        "#print(dataset_neg_tokenize[:2])\r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03xpkRzRqPpv"
      },
      "source": [
        "##Enlever les stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_HLAh_FIpPE"
      },
      "source": [
        "\r\n",
        "def NoStopWord(data):\r\n",
        "  dataset_nostopword=[]\r\n",
        "  for tab in data:\r\n",
        "    dataset_nostopword.append([w for w in tab if not w in list(nltk.corpus.stopwords.words('english'))])\r\n",
        "  return dataset_nostopword\r\n",
        "\r\n",
        "dataset_pos_without_stopwords = NoStopWord(dataset_pos_tokenize)\r\n",
        "dataset_neg_without_stopwords = NoStopWord(dataset_neg_tokenize)\r\n",
        "\r\n",
        "#print(dataset_pos_without_stopwords[:2])\r\n",
        "#print(dataset_neg_without_stopwords[:2])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEHdDCZRqR9n"
      },
      "source": [
        "##Stemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rC6Xqpc-aS6"
      },
      "source": [
        "\r\n",
        "def Stem(data):\r\n",
        "  stemmer = nltk.stem.snowball.EnglishStemmer()\r\n",
        "  dataset_pos_stem=[]\r\n",
        "  for i in range(len(data)):\r\n",
        "    dataset_pos_stem.append([stemmer.stem(w) for w in data[i]])\r\n",
        "\r\n",
        "  return dataset_pos_stem\r\n",
        "\r\n",
        "dataset_pos_stem = Stem(dataset_pos_without_stopwords)\r\n",
        "dataset_neg_stem = Stem(dataset_neg_without_stopwords)\r\n",
        "\r\n",
        "\r\n",
        "#print(dataset_pos_stem[:2])\r\n",
        "#print(dataset_neg_stem[:2])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIi61yOkqUaK"
      },
      "source": [
        "##Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Dv4E0S_My0"
      },
      "source": [
        "\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "def Lem(data):\r\n",
        "  Word_Lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "  dataset_pos_lema=[]\r\n",
        "  for i in range(len(data)):\r\n",
        "    dataset_pos_lema.append([Word_Lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in data[i]])\r\n",
        "  \r\n",
        "  return dataset_pos_lema\r\n",
        "\r\n",
        "dataset_pos_lema = Lem(dataset_pos_stem)\r\n",
        "dataset_neg_lema = Lem(dataset_neg_stem)\r\n",
        "\r\n",
        "#print(dataset_pos_lema[:2])\r\n",
        "#print(dataset_neg_lema[:2])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCXn5o8ZqXJ9"
      },
      "source": [
        "##Set des mots uniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEJmC6ErMk6t"
      },
      "source": [
        "#Mes tweets positifs se retrouvent en premier, dans le dataset c'est l'inverse, pas d'inquiétude\r\n",
        "dataset_lema= dataset_pos_lema + dataset_neg_lema\r\n",
        "\r\n",
        "def UniquePos(data):\r\n",
        "  tab_unique=set()\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      tab_unique.add(data[i][j])\r\n",
        "  return tab_unique\r\n",
        "\r\n",
        "tab_unique  = UniquePos(dataset_lema)\r\n",
        "#print(len(tab_unique),tab_unique)\r\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QcoabOnqfh-"
      },
      "source": [
        "#Transformation en vecteurs (bag of words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWDfS5m_9_iW"
      },
      "source": [
        "\r\n",
        "def VecteurData(data):\r\n",
        "  tab_vecteur=[]\r\n",
        "  Vecteur=[]\r\n",
        "\r\n",
        "\r\n",
        "  for j in range(len(data)):\r\n",
        "    for word in tab_unique:\r\n",
        "      if word in data[j]:\r\n",
        "        Vecteur.append(data[j].count(word))\r\n",
        "      else:\r\n",
        "        Vecteur.append(0)\r\n",
        "    tab_vecteur.append(Vecteur)\r\n",
        "    Vecteur=[]\r\n",
        "  return tab_vecteur\r\n",
        "\r\n",
        "tab_vecteur = VecteurData(dataset_lema)\r\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReXbiwKBqnMi"
      },
      "source": [
        "##Tableau des sentiments et map des targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAOUZJ0hBzTv",
        "outputId": "a4222e6a-6e88-4295-c469-6d19366a2af9"
      },
      "source": [
        "\r\n",
        "#Si le sentiment a pour valeur 4, c'est qu'il est positif, on le change donc en 1 pour notre régression\r\n",
        "y = np.array(list(map(lambda x: 1 if x==4 else 0, dataset_sentiment)))\r\n",
        "print(len(y),y)\r\n",
        "\r\n",
        "x=tab_vecteur"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000 [1 1 1 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNvbIbWi6Z57"
      },
      "source": [
        "##Sauvegarde du traitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47uRFexa6c3G"
      },
      "source": [
        "\r\n",
        "#pickle_out = open(\"/content/gdrive/MyDrive/Projet_NLP/Regression_Logistique/x.pickle\",\"wb\")\r\n",
        "#pickle.dump(x, pickle_out)\r\n",
        "#pickle_out.close()\r\n",
        "\r\n",
        "#pickle_out = open(\"/content/gdrive/MyDrive/Projet_NLP/Regression_Logistique/y.pickle\",\"wb\")\r\n",
        "#pickle.dump(y, pickle_out)\r\n",
        "#pickle_out.close()\r\n",
        "\r\n",
        "#pickle_out = open(\"/content/gdrive/MyDrive/Projet_NLP/Regression_Logistique/tab_unique.pickle\",\"wb\")\r\n",
        "#pickle.dump(tab_unique, pickle_out)\r\n",
        "#pickle_out.close()\r\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQF70bs2rbaB"
      },
      "source": [
        "##Fonction de traitement de nos futurs inputs (pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE5Yt39Vc3aT"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def traitement(my_string,tab_unique):\r\n",
        "\r\n",
        "  tokenizer = nltk.RegexpTokenizer(r'\\w+')\r\n",
        "  tokens = tokenizer.tokenize(my_string.lower())\r\n",
        "  tokens = [w for w in tokens if not w in list(nltk.corpus.stopwords.words('french'))]\r\n",
        "\r\n",
        "  stemmer = nltk.stem.snowball.EnglishStemmer()\r\n",
        "  Word_Lemmatizer = WordNetLemmatizer()\r\n",
        "  tokens_stem = [stemmer.stem(w) for w in tokens]\r\n",
        "\r\n",
        "  tokens_lema = [Word_Lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokens_stem]\r\n",
        "\r\n",
        "\r\n",
        "  Vecteur=[]\r\n",
        "  \r\n",
        "  for word in tab_unique:\r\n",
        "    if word in tokens_lema:\r\n",
        "      Vecteur.append(tokens_lema.count(word))\r\n",
        "    else:\r\n",
        "      Vecteur.append(0)\r\n",
        "  \r\n",
        " \r\n",
        "\r\n",
        "  return Vecteur\r\n",
        "\r\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmgBNUcaRRlZ"
      },
      "source": [
        "#Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx3NIdc5OgrG"
      },
      "source": [
        "\r\n",
        "#pickle_in = open(\"/content/gdrive/MyDrive/Projet_NLP/Regression_Logistique/x.pickle\",\"rb\")\r\n",
        "#X = pickle.load(pickle_in)\r\n",
        "\r\n",
        "#pickle_in = open(\"/content/gdrive/MyDrive/Projet_NLP/Regression_Logistique/y.pickle\",\"rb\")\r\n",
        "#Y = pickle.load(pickle_in)\r\n",
        "\r\n",
        "#pickle_in = open(\"/content/gdrive/MyDrive/Projet_NLP/Regression_Logistique/tab_unique.pickle\",\"rb\")\r\n",
        "#tab_unique = pickle.load(pickle_in)\r\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhW2P-FxRVic"
      },
      "source": [
        "#Split de la data et entrainement du modèle de regression logistique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSNcIlRTOUyV",
        "outputId": "b2f1520c-216a-4bb1-a168-7fa5700fd830"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.8,random_state=0)\r\n",
        "\r\n",
        "modele_regLog = linear_model.LogisticRegression(random_state = 0,solver = 'liblinear', multi_class = 'auto')\r\n",
        "\r\n",
        "modele_regLog.fit(x_train,y_train)\r\n",
        "\r\n",
        "precision = modele_regLog.score(x_test,y_test)\r\n",
        "print(precision*100)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72.625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmYlogzXRgbn"
      },
      "source": [
        "#Récupération de data pour tester notre modèle grâce à l'API Yelp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWk6_Dj6QGTh",
        "outputId": "1aa0affe-ad84-43b6-f5fb-a46716756565"
      },
      "source": [
        "api_key='q1HJoEmRMdypie_E6lYwiih0hNfuSatSTQXH6Bh9fnltm30JX4XO1TSCd6NARRVszrs4lMXb7N3i6LWB_UIBEinyG_ah-0expG6GUnzTdUss2gvNQRcJ7fJhK5g2YHYx'\r\n",
        "headers = {'Authorization': 'Bearer %s' % api_key}\r\n",
        "\r\n",
        "\r\n",
        "url = \"https://api.yelp.com/v3/businesses/FEVQpbOPOwAPNIgO7D3xxw/reviews\"\r\n",
        "req = requests.get(url, headers=headers)\r\n",
        "print('the status code is {}'.format(req.status_code))\r\n",
        "\r\n",
        "\r\n",
        "data=json.loads(req.text)\r\n",
        "data\r\n",
        " "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the status code is 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'possible_languages': ['fr', 'en', 'zh', 'pt', 'de', 'it', 'sv', 'ja', 'es'],\n",
              " 'reviews': [{'id': 'qOiuCTz_Gzgmose1GHaAlg',\n",
              "   'rating': 5,\n",
              "   'text': \"Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\",\n",
              "   'time_created': '2020-12-19 07:52:35',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=qOiuCTz_Gzgmose1GHaAlg&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': 'FRKnLBnlFiasr1Dc8oOIGQ',\n",
              "    'image_url': 'https://s3-media3.fl.yelpcdn.com/photo/1wm0yjWSw92j_ZsUFZBGzQ/o.jpg',\n",
              "    'name': 'Sarah G.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=FRKnLBnlFiasr1Dc8oOIGQ'}},\n",
              "  {'id': '7d56A_ObMPHHyywcfhnrUw',\n",
              "   'rating': 5,\n",
              "   'text': 'Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...',\n",
              "   'time_created': '2021-02-25 19:56:35',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=7d56A_ObMPHHyywcfhnrUw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': '77MIRZBZqbQKoonun8qWjQ',\n",
              "    'image_url': 'https://s3-media2.fl.yelpcdn.com/photo/ptL063kjqRUDQ6pny96iqg/o.jpg',\n",
              "    'name': 'Ipsita D.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=77MIRZBZqbQKoonun8qWjQ'}},\n",
              "  {'id': 'SYykUNBT2dJznMV5PpRnhg',\n",
              "   'rating': 5,\n",
              "   'text': 'Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...',\n",
              "   'time_created': '2020-12-12 15:19:49',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=SYykUNBT2dJznMV5PpRnhg&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': 'yw2cJk_SfGZlcoZKEUevxw',\n",
              "    'image_url': 'https://s3-media1.fl.yelpcdn.com/photo/EH2WTffzgKs72GHqa4kn6A/o.jpg',\n",
              "    'name': 'Thomas V.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=yw2cJk_SfGZlcoZKEUevxw'}}],\n",
              " 'total': 5609}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlgakihmSlZ3"
      },
      "source": [
        "#Prédiction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5V1WFttQJdr",
        "outputId": "43ed7ae5-1a88-4fb6-8889-013077eb2e6a"
      },
      "source": [
        "\r\n",
        "#Pour la première phrase test, je prends le premier élément de ma requête,\r\n",
        "#mais pour les deux autres j'ai copier coller les reviews complètes car l'API ne renvoie qu'une partie du texte, je voulais tester sur quelque chose de plus long\r\n",
        "\r\n",
        "str1 =data[\"reviews\"][0][\"text\"]\r\n",
        "phrase_pos_test=traitement(str1,tab_unique)\r\n",
        "\r\n",
        "#la str2 ne vient pas de la requête mais du site où str1 est prise, comme ça je peux cibler une review négative\r\n",
        "#str2=data[\"reviews\"][1][\"text\"]\r\n",
        "str2=\"Fool me once shame on you, fool me twice shame on me. We decided to place an order (30 wings) for the Super Bowl this year....bad idea.After 2.5 hours with the phone lines busy we got a call, asking if we still wanted our order.I then remembered we placed an order the year prior and never got our wings from Atomic because we got a message, they ran out of wings, similar to this year.Thanks but no thanks.\"\r\n",
        "phrase_neg_test=traitement(str2,tab_unique)\r\n",
        "\r\n",
        "#str3=data[\"reviews\"][2][\"text\"]\r\n",
        "str3=\"Buying beware...these are addicting.Still the best doughnuts in my opinion in the city. Ginormous and fluffy like a pillow but somehow still a substantial bite.All doughnuts are scrumptious but greatly enjoyed the Dulce De De Leche in the batch we tried the best.Big win, you can order on Seamless for delivery also on the website they say Caviar, Doordash, Uber Eats and Grub Hub and Sweetist are among other sites for delivery as well.\"\r\n",
        "phrase_pos2_test=traitement(str3,tab_unique)\r\n",
        "\r\n",
        "a = {0:'Négatif', 1:'Positif'}\r\n",
        "print(a)\r\n",
        "\r\n",
        "\r\n",
        "prediction_review = modele_regLog.predict([phrase_pos_test,phrase_neg_test,phrase_pos2_test])\r\n",
        "print(prediction_review)\r\n",
        "\r\n",
        "#NORMALEMENT POSITIF\r\n",
        "print(str1 + \": \",a[prediction_review[0]])\r\n",
        "#NORMALEMENT NEGATIF\r\n",
        "print(str2 + \": \",a[prediction_review[1]])\r\n",
        "#NORMALEMENT POSITIF\r\n",
        "print(str3 + \": \",a[prediction_review[2]])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'Négatif', 1: 'Positif'}\n",
            "[1 0 1]\n",
            "Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...:  Positif\n",
            "Fool me once shame on you, fool me twice shame on me. We decided to place an order (30 wings) for the Super Bowl this year....bad idea.After 2.5 hours with the phone lines busy we got a call, asking if we still wanted our order.I then remembered we placed an order the year prior and never got our wings from Atomic because we got a message, they ran out of wings, similar to this year.Thanks but no thanks.:  Négatif\n",
            "Buying beware...these are addicting.Still the best doughnuts in my opinion in the city. Ginormous and fluffy like a pillow but somehow still a substantial bite.All doughnuts are scrumptious but greatly enjoyed the Dulce De De Leche in the batch we tried the best.Big win, you can order on Seamless for delivery also on the website they say Caviar, Doordash, Uber Eats and Grub Hub and Sweetist are among other sites for delivery as well.:  Positif\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}